An aim for this project is to create a tool for super resolution microscopy that is moduler, therefore this document will outline how one could go about changing the current code to suit their needs.

# How to use the code





# 1 Fitting

As the fitting functions for this project, we have used GPUfit in order to find accurate locations for the centroids of blobs in each individual images. This is an algorithm that will run extremely quickly
but requires initial estimates for a rough estimate of where each centroid is in the image. 

    In order to calculate this we used a mean and standard deviation filter and then fed this into a clustering algorithm to label each pixel as a blob or not and 
    then found an estimate for the centroid using the mean of the pixels in each blob.

In order to run this on your computer, you will need a CUDA enabled GPU. You will also need to install the GPUfit package which is outlined in the requirements section.

If you would like to change this method, you can go to the main.py file and change the section that is marked
as Fitting. The getblob function performs the initial estimates and the GPUfit function performs the fitting. The required output from
this section is an array called parameters, this contains the centroid information in the form of:
            [Intensity, x, y, sigma, background]

NOTE:
    Units of intensity is photons this is converted
    Units of x and y is pixels
    Units of sigma is pixels
    Units of background is photons per pixel

.   1.1

        The localisation precision calculation is included in this section of the code. This is calculated by using 
        this formula for each parameter:
            localisation precision = sqrt(1/curvature of the log likelihood function)

        This can be changed by changing the function in the main.py file called localisation_precision. This function returns an array
        called chosen of the same length as the parameters array. This array contains the localisation precision for each parameter.
        It is of the form:
            [Frame No, Intensity, x, y, sigma, precision]

        NOTE:
            Units of intensity is photons
            Units of x and y is nm
            Units of sigma is nm
            Units of precision is ...


# 2 Drift Correction

In our code we have 3 different methods of drift correction, being BEADS, RCC & DME.

We can see that beads comes before we filter the data but the other methods come after. They all work on the same premise:
    INPUT: Array of localisations, hyperparameters for the method
    OUTPUT: Array of localisations with drift corrected

    NOTE: Beads also outputs a flag called success so that if it fails we can use a different method

The filtering process:
    We first make sure we only take converged localistions then
    We filter out localisations that are too close to the edge of the image then:
        1. Filter out localisations that are not within the bounds of the image
        2. Filter out based on localisation precision
        3. Filter out based on sigma

    Stages 2 and 3 are changes using the hyperparameters in parameters.txt


    2.1 Grouping
    
    Grouping simply looks into particles that are in the same location but on different frames and groups them together. This 
    takes the input of the array of localisations and outputs an array of grouped localisations.


# 3 Reconstruction

The code for Reconstruction is run by the reconstruction method which takes as input the array of localications as well is the 
beginning frames and some other hyperparameters and results in a tiff reconstructed image that will be saved.

This does not change any variables and simply will create the image into a variable called imageSR_proc and this is then saved into
the correct directory.

Additionally at the end of this is an optional stage to save all the localisation data


# 4 clustering

For this we use the DBSCAN algorithm which is a density based clustering algorithm. This takes as input the array of localisations and
various other hyperparameters and does not change any variables but creates a new file which is called the cluster profile.
This can easily be changed into calling a different clustering algorithm by changing the function in the main.py file called clustering.




# 5 Sorting

This is an optional stage that can be used to sort the localisations into different clusters. This is done by using the cluster profile and the 
Plates.xlsx file. This will output a text file with all the localisations sorted into different clusters. 

This can be changed by changing the function in the main.py file called sorting. 


